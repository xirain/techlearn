---
title: Skill-Inject：LLM Agent 技能文件注入攻击与防御
description: 研究 LLM Agent 的新兴安全威胁——基于技能文件（skill files）的提示注入攻击，介绍 SkillInject 基准测试及其 80% 攻击成功率的惊人发现
date: 2026-03-02
categories: [AI安全]
tags: [LLM, Agent, Prompt Injection, Security, Skill Inject, AI安全]
---

# Skill-Inject：LLM Agent 技能文件注入攻击与防御

## 快速摘要 (TL;DR)

> 这篇论文研究了一个新兴的安全威胁：基于 Agent 技能文件（skill files）的提示注入攻击。随着 LLM Agent 引入"Skills"(技能) 功能来扩展能力，攻击者可以在第三方代码/知识/指令中植入恶意提示，从而控制 Agent。作者提出了 SkillInject 基准测试，包含 202 个攻击-任务对，测试前沿 LLM 模型的安全性和可用性。结果令人担忧：攻击成功率高达 80%，Agent 会被诱骗执行数据泄露、破坏性操作甚至勒索软件行为。研究表明这个问题无法通过模型 scaling 或简单输入过滤解决，需要上下文感知的授权框架。

---

## 背景与威胁概述

### LLM Agent 的发展

LLM Agent 正在快速演进，其能力建立在三大支柱之上：

- **代码执行能力**：Agent 可以运行代码完成复杂任务
- **工具调用能力**：Agent 可以使用外部工具和 API
- **Skills 技能扩展机制**：这是最近引入的功能，允许用户扩展 LLM 应用

### 什么是 Agent Skills？

Skills 是 LLM 应用的一种扩展机制，允许用户通过加载第三方代码、知识库和指令来为 Agent 添加新能力。

**简单理解**：想象你的手机可以通过下载"插件"来获得新功能——比如下载一个翻译插件就能翻译外文。LLM Agent 的 Skills 机制类似，开发者可以上传包含 specialized 代码和指令的"技能包"，用户加载后 Agent 就能执行特定任务。

### 供应链安全问题

但问题来了：如果有人上传了一个看似有用的"翻译插件"，里面却藏着恶意指令呢？

Skills 机制打开了 LLM Agent 的供应链攻击大门：

- 攻击者可以在第三方代码中植入恶意提示
- 攻击者可以在知识库中隐藏恶意指令
- 用户加载技能时自动中招，难以察觉

这比传统的 prompt injection 更加隐蔽，因为恶意指令已经作为技能的一部分被嵌入，具有"合法"身份。

---

## SkillInject 基准测试

### 攻击类型分类

论文将攻击分为两类：

1. **明显恶意型注入**：直接要求执行有害操作，如"删除所有文件"
2. **隐蔽上下文依赖型注入**：隐藏在看似合法的指令中，利用上下文触发恶意行为

### 评估维度

- **安全性**：Agent 是否拒绝执行有害指令
- **可用性**：Agent 是否正确执行合法指令
- **攻击成功率**：恶意指令被执行的比例

---

## 实验结果

### 攻击成功率

实验结果令人震惊：

| 指标 | 结果 |
|------|------|
| 最高攻击成功率 | **80%** |
| 测试模型数量 | 多个前沿 LLM |
| 覆盖攻击类型 | 202 个注入-任务对 |

### 具体攻击后果

- **数据泄露 (Data Exfiltration)**：窃取用户敏感数据
- **破坏性操作 (Destructive Action)**：删除或损坏数据
- **勒索软件行为 (Ransomware-like Behavior)**：加密用户数据并索要赎金

---

## 关键发现与防御挑战

### 为什么传统防御无效？

#### 模型 Scaling 无法解决

实验表明，增加模型规模（更多参数）并不能显著降低攻击成功率。简单地"把模型做大"不是解决方案。

#### 简单输入过滤不足

传统防御关注用户直接输入的内容，可以通过关键词过滤、语义分析等手段检测。但技能文件注入发生在"预加载"阶段——恶意指令已经作为技能的一部分被嵌入，当 Agent 加载技能时，这些指令就获得了"合法"身份。

### 需要上下文感知授权框架

解决方案的核心是：在指令执行前评估风险

1. **评估指令来源**：是用户直接请求还是技能文件引入？
2. **分析指令意图**：是否涉及敏感操作如数据外发？
3. **请求用户确认**：高风险操作需要用户授权
4. **记录审计日志**：追踪所有执行的操作

这类似于手机应用申请权限时的"授权"机制——不是完全禁止，而是让用户/系统知道风险并做出决策。

---

## 批判性思考

### Q：为什么传统针对用户输入的提示注入过滤无法防御技能文件注入？

**传统防御**关注用户直接输入的内容，可以通过关键词过滤、语义分析等手段检测。

**技能文件注入**发生在"预加载"阶段——恶意指令已经作为技能的一部分被嵌入，当 Agent 加载技能时，这些指令就获得了"合法"身份。Agent 很难区分"系统自带的指令"和"恶意植入的指令"。

### Q：80% 的攻击成功率是否意味着所有前沿模型都不安全？

这个数字反映的是"在当前防御机制下"的表现，说明现有技术存在系统性漏洞。不同模型的脆弱程度可能不同，而且这个基准测试针对的是"技能文件注入"这一特定攻击向量。

关键启示是：**需要重新设计 Agent 的安全架构**，而非单纯依赖模型本身的防御能力。

---

## 一句话记住

> **技能文件注入 = LLM Agent 的供应链攻击——攻击者预先在第三方扩展中埋雷，用户加载即中招，模型 scaling 和输入过滤都防不住。**

---

## 延伸阅读

- **论文官网**：https://www.skill-inject.com/
- **arXiv 链接**：https://arxiv.org/abs/2602.20156
- **相关主题**：传统 Prompt Injection 攻击与防御
- **相关主题**：LLM Agent 安全架构设计
- **相关主题**：AI Red Teaming 方法论

---

## 参考

- Schmotz, D., Beurer-Kellner, L., Abdelnabi, S., & Andriushchenko, M. (2026). Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks. arXiv:2602.20156
